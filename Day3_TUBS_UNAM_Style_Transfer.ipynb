{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axelsavrov/ML-for-Medical-Students/blob/main/Day3_TUBS_UNAM_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://www.plri.de/assets/images/logo_plri_de.png\"\n",
        "width=\"200\">\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Siegel_TU_Braunschweig_transparent.svg/800px-Siegel_TU_Braunschweig_transparent.svg.png\"\n",
        "width=\"250\">\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3d/Medizinische_Hochschule_Hannover_logo.svg\"\n",
        "width=\"250\">\n",
        "<img src=\"https://images.seeklogo.com/logo-png/38/2/universidad-nacional-autonoma-de-mexico-unam-logo-png_seeklogo-387361.png\" width=\"110\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "EAkFzWb4EEUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"Teal\" face=\"Georgia,arial\">\n",
        "  <h1 align=\"center\"><b></b></h1>\n",
        "  <h1 align=\"center\"><i><b>Style Transfer</b></i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Georgia,arial\">\n",
        "  <h5 align=\"center\"><i><b>Voluntarios de TUBS</b></i></h5>\n",
        "  <h5 align=\"center\"><i><b>mail: unam.tubs@gmail.com</b></i></h5>\n",
        "</font>\n",
        "\n",
        "**CONTENTS**\n",
        "\n",
        "Neural Style Transfer: The main idea is to combine the content of a ‚Äúreal‚Äù image with the artistic style of another image.\n",
        "This process is based on deep neural networks, which are capable of extracting the content of one image (shapes, structure) and the style of another (colors, textures, strokes), and merging them into a new image.\n",
        "\n",
        "- Style_Transfer.ipynb: Main notebook where all the code is ready to be executed step by step.\n",
        "- Image gallery.\n",
        "\n",
        "----------------------------------------\n",
        "\n",
        "**üöÄ HOW TO USE THE CODE?**\n",
        "\n",
        "1. Open the Style_Transfer.ipynb notebook in Google Colab.\n",
        "\n",
        "* Go to the menu: File > Save a copy to Drive\n",
        "* Gray cells with code are executed with Shift + Enter\n",
        "\n",
        "----------------------------------------\n",
        "\n",
        "**üîß HOW TO RUN THE NOTEBOOK?**\n",
        "\n",
        "1. Locate the block of code you want to run.\n",
        "In the upper left corner of the block, click on the triangle button ‚ñ∂Ô∏è\n",
        "\n",
        "2. Wait for a green check mark ‚úÖ to appear next to the block: this indicates that it has finished executing.\n",
        "\n",
        "----------------------------------------\n",
        "\n",
        "**üìù ADDITIONAL NOTES**\n",
        "\n",
        "- The code may take between 10 and 15 minutes to load.\n",
        "\n",
        "----------------------------------------\n",
        "**üôã IF YOU HAVE ANY QUESTIONS ABOUT HOW TO USE THE CODE, YOU CAN SEND US AN EMAIL**\n",
        "\n",
        "Email for questions -- unam.tubs@gmail.com\n",
        "\n",
        "Attach a screenshot if you get an error message.\n",
        "\n"
      ],
      "metadata": {
        "id": "3ihWp3SjEIPd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLbq2eDC5zZ8"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlVnO5y5RQ2"
      },
      "source": [
        "__Import libraries used by thisnotebook.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9qyjO-dhHWu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV_LeFP3jmLt"
      },
      "source": [
        "__Select computation device accordingly to what is available.__\n",
        "\n",
        "If this machine have a gpu (cuda is available) we use it. Otherwise our fallback is the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYkwmoCEhKUc"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BWrgk_Jyubf"
      },
      "source": [
        "__Hyperparameters__\n",
        "\n",
        "Hyperparameters control the number of iteration, size of image, and relative ration between losses (content, style, and denoising).\n",
        "\n",
        "Fine tuning this parameter will affect the final result of the picture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHznEJM1ysZW"
      },
      "source": [
        "ITERATIONS = 20 # Number of iterations\n",
        "IMAGE_WIDTH = 512 # 512x512 is fine for HD images\n",
        "IMAGE_HEIGHT = 512\n",
        "CONTENT_WEIGHT = 1 # Weight of the content loss\n",
        "STYLE_WEIGHT = 0.01 # Weight og the style loss\n",
        "TOTAL_VARIATION_WEIGHT = 15 # Weight of the total variation loss\n",
        "INPUT_IMAGE = 'noise' # Can be : 'noise', 'content', 'style', or a path to a file."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMWKPFs0Z1FV"
      },
      "source": [
        "Thw next parameters are the heart of the algorithm, They deeply affect the result of combining style and content, because they define what IS the style and content.\n",
        "\n",
        "To compute style and content, we use a deep neural network called VGG. Here is how VGG looks like inside.\n",
        "![VGG Layers](https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg)\n",
        "\n",
        "Tu compute the style of an image, we run the image through VGG. Then, we look at what values flowed across the layers (named _conv1_1_, _conv1_2_, etc.) and keep some of them.\n",
        "\n",
        "For the __content__, we usually keep the first layers. The last convolutional layer from the second block is always a good bet.\n",
        "\n",
        "For the __style__, a mix of the first conv layers (they contain color and texture informations) and last layers (they contain complexe features like trees, shapes, eyes, etc...) gives the best results.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "__The names of layers you can add the the following lists in order to define the style and content loss are:__\n",
        "  * `conv_n` where `n` is the index of the convolutional layer from the input.\n",
        "  * `relu_n` where `n` is the index of the rectified linear layer from the input.\n",
        "  * `pool_n` where `n` is the index of the max pooling layer from the input.\n",
        "  * `bn_n` zhere `n` is the index of the batch normalization layer from the input.\n",
        "  \n",
        "  \n",
        "  For example, convolutional layers are indexed from `conv_1` to `conv_16`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKZOYy1vbk99"
      },
      "source": [
        "content_layers = ['conv_1', 'conv_2', 'conv_4']\n",
        "style_layers = ['conv_2', 'conv_3', 'conv_4', 'conv_7', 'conv_10', 'conv_8']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8CCdZvW78FW"
      },
      "source": [
        "Names under which the images will be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz7iMsfl76wK"
      },
      "source": [
        "content_image_path = \"content.png\"\n",
        "style_image_path = \"style.png\"\n",
        "output_image_path = \"result.png\"\n",
        "combined_image_path = \"combined.png\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oEiBDWg-H6U"
      },
      "source": [
        "# Image selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsfnX3wPjsBO"
      },
      "source": [
        "### Images\n",
        "\n",
        "We download images from the web, but you can also specify the path to an image stored on your computer.\n",
        "\n",
        "How do you do this?\n",
        "Just drag and drop your file into the menu on the left, under the ‚ÄúFiles‚Äù tab, and then type the file name (including the extension) instead of the URL.\n",
        "\n",
        "Note:\n",
        " If you want to upload your own images, here you can see step by step how to do it. The link will be in the part of the code where it says ‚Äú#my_content_image.‚Äù Just uncomment that line (by removing the # symbol) and write the path to your image.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D95t7NRhhNWc"
      },
      "source": [
        "# Content images\n",
        "san_francisco = \"https://www.economist.com/sites/default/files/images/print-edition/20180602_USP001_0.jpg\"\n",
        "cat = \"https://media.istockphoto.com/id/1443562748/es/foto/lindo-gato-de-jengibre.jpg?s=612x612&w=0&k=20&c=JVC5Z3LxpaTQaXu_fMZjIb73r39z6b0SnAxvNI8iZG0=\"\n",
        "dog = \"https://raw.githubusercontent.com/gaganbahga/cosine_art/master/test_images/test_image_2.jpg\"\n",
        "doge = \"https://raw.githubusercontent.com/codazzo/dogr/master/public/doge.png\"\n",
        "#my_content_image = \"/content/imagen_contenido.jpg\"\n",
        "\n",
        "# Style images:\n",
        "tytus = \"http://meetingbenches.com/wp-content/flagallery/tytus-brzozowski-polish-architect-and-watercolorist-a-fairy-tale-in-warsaw/tytus_brzozowski_13.jpg\" #Warsaw by Tytus Brzozowski, http://t-b.pl\n",
        "picasso_blue = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQAvqs_gleJBIg3673vZDnuKS-QsGA60JlniA&s\"\n",
        "elephant = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQyz-jrx9Na9NZg0oBAx50m7ahJ5VO2yUSvKCCexiNzi0z98yfp\"\n",
        "colorswir = \"https://paintings.pinotspalette.com/colorful-swirly-sky-tv.jpg?v=10026315\"\n",
        "scream = \"https://painting-planet.com/images/1/image046.jpg\"\n",
        "#my_style_image = \"/content/imagen_estilo.jpg\"\n",
        "\n",
        "#my_style_image =\n",
        "\n",
        "\n",
        "# The function wich allow us to download / open an image\n",
        "def open_image(url):\n",
        "  if url.startswith(\"http\"):\n",
        "    return Image.open(BytesIO(requests.get(url).content))\n",
        "  else:\n",
        "    return Image.open(url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKLmm-Ri92iG"
      },
      "source": [
        "This is where you select your __Content image__ and __Style Image__.\n",
        "\n",
        "It will be more fun if you use your own images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3-l80fg849s"
      },
      "source": [
        "content_url = my_content_image\n",
        "style_url = my_style_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZcjSi6CsdpU"
      },
      "source": [
        "### Image loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPZ73PwUhh7A"
      },
      "source": [
        "#@title This is the _content_ image\n",
        "content_image = open_image(content_url)\n",
        "content_image = content_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "content_image.save(content_image_path)\n",
        "content_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxBsYWmZhiB3"
      },
      "source": [
        "#@title This is the _style_ image\n",
        "style_image = open_image(style_url)\n",
        "style_image = style_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "style_image.save(style_image_path)\n",
        "style_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it2Bxcu6jblV"
      },
      "source": [
        "# Build the algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8VGtzHG-PPu"
      },
      "source": [
        "## Feature loss and total variation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1slVvnQqR20o"
      },
      "source": [
        "### Feature losses\n",
        "\n",
        "The feature loss is a loss based on which features a neural network find inside an image. We have two such losses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-d8lMqj9RN"
      },
      "source": [
        "The first loss is the content loss, based on what content an image contain.\n",
        "It is basically like a Mean Square Error. But instead of computing the MSE on the content image's pixels and the computed image's pixels, we compute the MSE between the activation of some layers of VGG19 after running our images through it.\n",
        "\n",
        "To compute the activation layer, we add a pytorch block named ContentLoss inside the VGG network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOaf38fhkQVV"
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "  def __init__(self, target,):\n",
        "    super(ContentLoss, self).__init__()\n",
        "    # We 'detach' the target content from the tree used\n",
        "    # to dynamically compute the gradient: this is a stated value,\n",
        "    # not a variable.\n",
        "    self.target = target.detach()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.loss = F.mse_loss(input, self.target)\n",
        "    return input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTY3rhVaSsIM"
      },
      "source": [
        "The second loss is the style loss. Instead of looking directly to activation of the layer, we first make a computation that compute somehow  the _angle_ between the features detected by the neural network.\n",
        "\n",
        "We compute the [gram matrix](https://en.wikipedia.org/wiki/Gramian_matrix), which is a matrix whose coefficients are the dot product of two features vectores in the same activation layer from a single image.\n",
        "\n",
        "We then have for each activation layer, two gram matrix (one from the style image, and one from the cotent image). We simply compute the MSE between the pairs of matrices.\n",
        "\n",
        "The following code allow to plug a layer in VGG19 retreiving the gram matrices from each activation layer of interest.\n",
        "\n",
        "---\n",
        "__Why do I call this values _angles_?__\n",
        "\n",
        "The Gram matrix is the matrix $G_{ij} = <v_i, v_j>$ is obtained by doing the dot product of each pair of feature vector. The dot product of two vectors $u_i$ and $v_i$ in $\\mathbb{R}^3$ can be expressed as $||u_i|| .  ||v_i|| . \\cos(u_i, v_i)$ where the two first terms are the length of the vectors, and the third is the cosine of the angle between this vectors.\n",
        "Therefor, you can see the value of this dot product as a function of the _angle between two features_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9XRfuaCk4MT"
      },
      "source": [
        "class StyleLoss(nn.Module):\n",
        "  def __init__(self, target_feature):\n",
        "    super(StyleLoss, self).__init__()\n",
        "    self.target = self.gram_matrix(target_feature).detach()\n",
        "\n",
        "  def forward(self, input):\n",
        "    G = self.gram_matrix(input)\n",
        "    self.loss = F.mse_loss(G, self.target)\n",
        "    return input\n",
        "\n",
        "  @staticmethod\n",
        "  def gram_matrix(input):\n",
        "    a, b, c, d = input.size()\n",
        "    # Here:\n",
        "    # a is the batch size(=1)\n",
        "    # b is the number of feature maps\n",
        "    # (c,d) are the dimensions of a feature map\n",
        "\n",
        "    # We reshape the activation layer into a collection of feature vectors\n",
        "    features = input.view(a * b, c * d)\n",
        "\n",
        "    # Compute the gram product\n",
        "    G = torch.mm(features, features.t())\n",
        "\n",
        "    # We 'normalize' the values of the gram matrix\n",
        "    # by dividing by the norm of gram matrix filled with ones.\n",
        "    return G.div((a * b * c * d) ** 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e97uvoldjson"
      },
      "source": [
        "### Total Variation\n",
        "\n",
        "The last loss is a _smoothing loss_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdI4DNpUezFc"
      },
      "source": [
        "\n",
        "\n",
        "This is called the total variation because it basically count the amplitude of all the variations happening in the image. Hight amplitude variation are caused by high constrats / brutal change of pixel values.\n",
        "\n",
        "Minimizing the total variation of an image can to reducing noise (it is called [total variation denoising](https://en.wikipedia.org/wiki/Total_variation_denoising)). But you have to be carefull, because an image with a minimal total variation is a uniformly colored square. And it would be a really boring and uninteresting picture.\n",
        "\n",
        "Too much total variation weight will lead to darker / less colorful images. But too few of it will generate very noisy images, without the \"brush painting\" style we are often loooking for in style transfer.\n",
        "\n",
        "Since this is computed only from the resulting image , we just need a function to compute it.\n",
        "\n",
        "---\n",
        "_How is computed total variation ?_\n",
        "\n",
        "Total variation is express by $\\sum_{i, j} \\sqrt{(x_{i+1, j}-x_{i, j})^2 + (x_{i+1, j}-x_{i, j})^2}$ where $x_{i, j}$ is the pixel at coordinates $(i, j)$. This is basically the euclidean norm of all the partial horizontal and vertical derivative of the surface given by $i, j \\mapsto x_{i, j}$.\n",
        "We are basically summing all the local derivative along the height map defined by the image.\n",
        "\n",
        "This formula have the nice property to give a value which is invariant (up to an error introduced by the discrete nature of the image) by rotation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eiCazwcBwY8"
      },
      "source": [
        "def total_variation_loss(x): # Expect a mini batch of dim NxWxH\n",
        "  image_height = x.shape[1]\n",
        "  image_width = x.shape[2]\n",
        "  dx = x[:, :image_height-1, :image_width-1, :] - x[:, 1:, :image_width-1, :]\n",
        "  dy = x[:, :image_height-1, :image_width-1, :] - x[:, :image_height-1, 1:, :]\n",
        "  loss = (dx ** 2 + dy ** 2).sum() ** 0.5\n",
        "  # Return loss normalized by image and batch size\n",
        "  return loss / (image_width * image_height * x.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I5lquFkjRFu"
      },
      "source": [
        "### Normalization\n",
        "\n",
        "We add one more block to automatically rescale and remove the mean of the images going through VGG19.\n",
        "\n",
        "While training neural network on image dataset, all imges are their mean removed, and values are rescaled according to the standard deviation of the dataset. This is a common processing which makes the neural network convergence easier. The only implication is that once we want to use this network, we have to normalise the input image according to the coefficients used during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaOReeMjk7Kl"
      },
      "source": [
        "# Create a module to normalize input image so we can easily put it in a\n",
        "# nn.Sequential\n",
        "class Normalization(nn.Module):\n",
        "  def __init__(self, mean, std):\n",
        "    super(Normalization, self).__init__()\n",
        "    # Reshape the mean and std to make them [C x 1 x 1] so that they can\n",
        "    # directly broadcast to image Tensor of shape [B x C x H x W].\n",
        "    # B is batch size. C is number of channels. H is height and W is width.\n",
        "    self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "    self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "  def forward(self, img):\n",
        "    # Normalize img\n",
        "    return (img - self.mean) / self.std\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0apWZopqAm1H"
      },
      "source": [
        "# Download and build VGG19\n",
        "\n",
        "We now load our pretrained neural network (VGG19) for computing the losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjKdVqQ7k5nS"
      },
      "source": [
        "# Neural network used.\n",
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "# Normalization mean and standard deviation.\n",
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqvqKyumo86p"
      },
      "source": [
        "We now build the actual network which will be called hundreads of times by the style transfer algorithm.\n",
        "\n",
        "We only keep the layers required to compute the feature loss, and throw everything else away. We interleave our Normalization, StyleLoss and ContentLoss layers between VGG19's layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzUkNGCSk-jt"
      },
      "source": [
        "def get_model_and_losses(style_img, content_img,\n",
        "                               cnn=cnn,\n",
        "                               cnn_normalization_mean=cnn_normalization_mean,\n",
        "                               cnn_normalization_std=cnn_normalization_std):\n",
        "  # We make a deep copy of vgg19 in order to not modify the original\n",
        "  cnn = copy.deepcopy(cnn)\n",
        "\n",
        "  # Normalization module\n",
        "  normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
        "\n",
        "  # This list will contain the losses computed by the network.\n",
        "  content_losses = []\n",
        "  style_losses = []\n",
        "\n",
        "  # We rebuild the model as a nn sequential whose first layer is\n",
        "  # our normalization layer.\n",
        "  model = nn.Sequential(normalization)\n",
        "\n",
        "  # We brows the layer of `cnn` and stack them into `model`.\n",
        "  i = 0  # Incremented every time we see a conv layer.\n",
        "  for layer in cnn.children():\n",
        "\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        i += 1\n",
        "        name = 'conv_{}'.format(i)\n",
        "    elif isinstance(layer, nn.ReLU):\n",
        "        name = 'relu_{}'.format(i)\n",
        "        # The in-place version doesn't play very nicely with the ContentLoss\n",
        "        # and StyleLoss we insert below. So we replace with out-of-place\n",
        "        # ones here.\n",
        "        layer = nn.ReLU(inplace=False)\n",
        "    elif isinstance(layer, nn.MaxPool2d):\n",
        "        name = 'pool_{}'.format(i)\n",
        "    elif isinstance(layer, nn.BatchNorm2d):\n",
        "        name = 'bn_{}'.format(i)\n",
        "    else:\n",
        "        raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
        "\n",
        "    model.add_module(name, layer)\n",
        "\n",
        "    # Check if the layer we just added was in the content layer list.\n",
        "    # If so, we just stack a Content Loss layer.\n",
        "    if name in content_layers:\n",
        "      target = model(content_img).detach()\n",
        "      content_loss = ContentLoss(target)\n",
        "      model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
        "      content_losses.append(content_loss)\n",
        "\n",
        "    # Check if the layer we just added was in the style layer list.\n",
        "    # If so, we just stack a Style Loss layer.\n",
        "    if name in style_layers:\n",
        "      target_feature = model(style_img).detach()\n",
        "      style_loss = StyleLoss(target_feature)\n",
        "      model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
        "      style_losses.append(style_loss)\n",
        "\n",
        "  # Now we trim off the layers after the last content and style losses\n",
        "  # to keep the model as small as possible.\n",
        "  for i in range(len(model) - 1, -1, -1):\n",
        "      if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "          break\n",
        "\n",
        "  model = model[:(i + 1)]\n",
        "\n",
        "  return model, style_losses, content_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFv0BZmtvE-p"
      },
      "source": [
        "# Running the algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfpjkZGwukYf"
      },
      "source": [
        "We write a function converting PIL images to a `torch.Tensor` and apply it to our input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijUw6Z4hhRuN"
      },
      "source": [
        "def image_to_tensor(image):\n",
        "  # Transform to tensor\n",
        "  image = transforms.ToTensor()(image)\n",
        "  # Fake batch dimension required to fit network's input dimensions\n",
        "  image = image.unsqueeze(0)\n",
        "  # Move to the right device and convert to float\n",
        "  return image.to(device, torch.float)\n",
        "\n",
        "content_img = image_to_tensor(content_image)\n",
        "style_img = image_to_tensor(style_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdM8BY9ku9tl"
      },
      "source": [
        "We now declare the inverse function, converting back tensor to images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UVgY8Ebix7Z"
      },
      "source": [
        "# Reconvert a tensor into PIL image\n",
        "def tensor_to_image(tensor):\n",
        "  img = (255 * tensor).cpu().detach().squeeze(0).numpy()\n",
        "  img = img.clip(0, 255).transpose(1, 2, 0).astype(\"uint8\")\n",
        "  return Image.fromarray(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo3TJ96Febl"
      },
      "source": [
        "We now generate or load the input image from which we are going to converge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gLqgtY_w5n3"
      },
      "source": [
        "#@title This is the _input_ image\n",
        "if INPUT_IMAGE == 'noise':\n",
        "  input_img = torch.randn(content_img.data.size(), device=device)\n",
        "elif INPUT_IMAGE == 'content':\n",
        "  input_img = content_img.clone()\n",
        "elif INPUT_IMAGE == 'style':\n",
        "  input_img = style_img.clone()\n",
        "else:\n",
        "  image = open_image(INPUT_IMAGE).resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "  input_img = image_to_tensor(image)\n",
        "  input_img += torch.randn(content_img.data.size(), device=device)*0.1\n",
        "\n",
        "# To visualise it better, instead of clipping values, we rescale\n",
        "# them to fit [-1,1], and convert to an image. This is mostly because\n",
        "# the visualization given is closer to what the actual values stored\n",
        "# in the tensor are.\n",
        "if INPUT_IMAGE == 'noise':\n",
        "  img = transforms.ToPILImage()(input_img[0].cpu())\n",
        "else:\n",
        "  img = tensor_to_image(input_img[0])\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KLF9uUqF53j"
      },
      "source": [
        "We have to choose an optimisation algorithm for our gradient descent.\n",
        "The algorithm giving the best visual result is LBFGS. But you can try other algorithms like Adam, RAdam, SGD..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJXOCuzSNlnF"
      },
      "source": [
        "def get_input_optimizer(input_img):\n",
        "  # This line tell LBFGS what parameters we should optimize\n",
        "  optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "  #optimizer = optim.Adam([input_img.requires_grad_()])\n",
        "  return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmRnB8S7GNa8"
      },
      "source": [
        "We add one more function in order to display the evolution of the input image while the algorithm run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "podEcetEHWSD"
      },
      "source": [
        "def show_evolution(tensor, history=[], title=None):\n",
        "    image = tensor.cpu().clone().squeeze(0)\n",
        "    image = tensor_to_image(image)\n",
        "    # Display a big figure\n",
        "    plt.rcParams['figure.figsize'] = [10, 5]\n",
        "    plt.figure()\n",
        "    # Image\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    # Losses\n",
        "    ax = plt.subplot(122)\n",
        "    plt.yscale('log')\n",
        "    plt.title('Losses')\n",
        "    import numpy as np\n",
        "    history = np.array(history).T\n",
        "    plt.plot(history[0], label='Style')\n",
        "    plt.plot(history[1], label='Content')\n",
        "    plt.plot(history[2], label='Variation')\n",
        "    plt.plot(history[3], label='Sum')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    # Finaly show the graph\n",
        "    plt.show()\n",
        "    # Display a textual message\n",
        "    print('Style Loss : {:4f} Content Loss: {:4f} Variation Loss: {:4f} Sum: {:4f}'.format(\n",
        "        history[0][-1], history[1][-1], history[2][-1], history[3][-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-CjJ2YEGWLI"
      },
      "source": [
        "We are now ready to converge toward our AI painted image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxUAI752lQU-"
      },
      "source": [
        "print('Building the style transfer model..')\n",
        "model, style_losses, content_losses = get_model_and_losses(style_img, content_img)\n",
        "optimizer = get_input_optimizer(input_img)\n",
        "\n",
        "print('Optimizing..')\n",
        "iterations = 0\n",
        "history=[]\n",
        "while iterations <= ITERATIONS * 100:\n",
        "  # Compute the loss and backpropagate to the input_image.\n",
        "  # (The LBFGS optimizer only accept work through closures.)\n",
        "  def closure():\n",
        "      global history\n",
        "      global iterations\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Compute the total variation loss\n",
        "      variation_score = total_variation_loss(input_img) * TOTAL_VARIATION_WEIGHT\n",
        "      # Compute the features through the model\n",
        "      model(input_img)\n",
        "      # Compute style and content losses\n",
        "      style_score = sum(sl.loss for sl in style_losses)\n",
        "      style_score *= STYLE_WEIGHT / len(style_losses)\n",
        "      content_score = sum(cl.loss for cl in content_losses)\n",
        "      content_score *= CONTENT_WEIGHT / len(content_losses)\n",
        "      # Our global loss is the sum of the 3 values\n",
        "      loss = style_score + content_score + variation_score\n",
        "      # Save the value of loss in order to draw them as a graph\n",
        "      history += [[style_score.item(), content_score.item(), variation_score.item(), loss.item()]]\n",
        "\n",
        "      # If the iteration is a multiple of 100, display some informations\n",
        "      iterations += 1\n",
        "      if iterations % 100 == 0:\n",
        "          show_evolution(input_img.data.clone().detach().clamp(0, 1), history, title=\"Iteration %d:\" % iterations)\n",
        "\n",
        "      # Backpropagate gradients and leave the optimizer do his job.\n",
        "      loss.backward()\n",
        "      return loss\n",
        "\n",
        "  optimizer.step(closure)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHFylwXN5RIv"
      },
      "source": [
        "#@title Our beautiful result\n",
        "img = tensor_to_image(input_img)\n",
        "img.save(output_image_path)\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msdndAkd0Mce"
      },
      "source": [
        "#@title Visualise combined results\n",
        "combined = Image.new(\"RGB\", (IMAGE_WIDTH*3, IMAGE_HEIGHT))\n",
        "x_offset = 0\n",
        "for image in map(Image.open, [content_image_path, style_image_path, output_image_path]):\n",
        "    combined.paste(image, (x_offset, 0))\n",
        "    x_offset += IMAGE_WIDTH\n",
        "combined.save(combined_image_path)\n",
        "combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g9G_h7_3W-pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Crear un tensor de 2 dimensiones (matriz)\n",
        "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"Tensor de 2 dimensiones:\")\n",
        "print(tensor_2d)\n",
        "print(\"\")\n",
        "\n",
        "# Crear un tensor de 3 dimensiones\n",
        "tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "print(\"Tensor de 3 dimensiones:\")\n",
        "print(tensor_3d)\n",
        "print(\"\")\n",
        "\n",
        "# Crear un tensor de 4 dimensiones\n",
        "tensor_4d = torch.tensor([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]], [[[9, 10], [11, 12]], [[13, 14], [15, 16]]]])\n",
        "print(\"Tensor de 4 dimensiones:\")\n",
        "print(tensor_4d)\n"
      ],
      "metadata": {
        "id": "fm6tr6LDW_A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**We want to see your creation!**\n",
        "\n",
        "##Share the results of your exercise by uploading your image at the following link. We would love to see how you combined content and style in your own unique way.\n",
        "\n",
        "##üëâ [Upload your image here](https://docs.google.com/forms/d/e/1FAIpQLSdawwLBC8Hmy0D-djRLu4uS-CwnyvOEr8ir4ovw9iBRgfvQ4w/viewform?usp=dialog)\n",
        "\n",
        "**Remember: each result is unique, and we can all learn from seeing each other's work. Go ahead and share it!**\n"
      ],
      "metadata": {
        "id": "5mUqWEI6zUTK"
      }
    }
  ]
}